---
title: "Morloc benchmarks"
author: "Zebulun Arendsee"
date: "2025-04-06"
output: html_document
---

<!-- $ Rscript -e "rmarkdown::render('plot.Rmd', output_format = 'html_document')" -->

Benchmarks using morloc v0.53.3

# Benchmark comparing morloc, Snakemake and Nextflow

```{r, echo=FALSE, message=FALSE}
require(ggplot2)
require(dplyr)
require(stringr)
require(broom)
library(ggpmisc)
require(patchwork)
library(knitr)
library(kableExtra)


df <- read.csv("stats-v0.53.3.csv", stringsAsFactors=FALSE)

# Set the language that is loading the input file
df$load_lang <- ifelse(df$parameter_lang == "morloc", str_replace(df$parameter_mode, "^.", ""), NA)
df$load_lang <- ifelse(df$parameter_lang == "snakemake" & df$parameter_mode == "testlt", "R", df$load_lang)
df$load_lang <- ifelse(df$parameter_lang %in% c("snakemake", "nextflow") & df$parameter_mode == "testlc", "python", df$load_lang)
df$load_lang <- ifelse(!(df$parameter_lang %in% c("snakemake", "nextflow", "morloc")), df$parameter_lang, df$load_lang)
df$load_lang <- tolower(ifelse(df$load_lang == "python", "p", df$load_lang))

# Set the language running the code
df$run_lang <- ifelse(df$parameter_lang == "morloc", str_replace(df$parameter_mode, ".$", ""), df$load_lang)

df$t <- df$min
df$n <- df$parameter_node
df$k <- df$parameter_size * 10
df$mode <- df$parameter_mode
df$lang <- df$parameter_lang

df <- select(df, command, t, n, k, mode, lang, load_lang, run_lang) 
```

The runtime performance of a component workflow can be modeled as follows: 

$$
t(n,k) = S + Lk + n(Q + Rk + Ik)
$$

where
 * $t$ - (s) total pipeline runtime
 * $n$ - (unit) number of nodes in the pipeline
 * $k$ - (Mb) size of the data that is passed between each node (assuming data size is the same between all nodes)
 * $S$ - (s) the one-time cost for initializing the pipeline
 * $L$ - (s/Mb) the one-time cost for loading an initial data file of size `k` 
 * $Q$ - (s) the constant cost of starting a node
 * $R$ - (s/Mb) the runtime of running node function per Mb of input
 * $I$ - (s/Mb) the cost for moving data to/from a node per Mb

The $L$ and $R$ terms, measures of the time required to load initial data and
run each core computation in ecah node, is mostly independent of the
workflow. Parallelism and scaling are of course also important to runtime. This
includes full utilization of individual machines and the distrubution of work
between machines. The model above could be generalized to describe parallelism.

The performance of workflow managers can be compared by estimating their $S$,
$Q$, and $I$ values.

The initial program start time, $S$, is a single constant added to total runtime
of a few seconds for Nextflow and tens to hundreds of milliseconds for morloc
and Snakemake. A few seconds of extra latency is not a major issue for
computations that run more than a few minutes. But this latency qualitatively
changes the field of use cases for the workflow language. For example, a few
seconds of latency means that the workflow is inappropriate for interactive use
in running small jobs as a CLI tool or on the backend of a web service. On a
CLI, a half second delay is painfully noticeable. This can be especially
irritating when requesting usage or version info from a tool and then having to
wait for a print statement. Still, for scientific computing, this is the least
important of the three metrics.

The node start time, $Q$, is a constant cost added for every node in the
pipeline. High $Q$ penalties become a bottlekneck when $Q$ is far less than
$R$. Essentially, a workflow manager with a high $Q$ penalty will be limited to
coarse workflows where every node is heavy computational step. So every node in
a high-$Q$ workflow will need to be engineered to have an appropriate amount of
work. This often involves manual chunking. Rather than writing functions that
handle the base case, each function must instead handle many input values and
then combine there results in a single output. This pushes logic into the
nodes. Rather than implementing the parallel compute strategy once at a
high-level in the pipeline code, parallelism must be implemented independently
in every program. Many high-$Q$ language don't even support loops. For morloc,
$Q$ is a few milliseconds for foreign calls and nearly zero cost for
within-language calls (just function call overhead). For Nextflow and Snakemake,
$Q$ is around 100ms. This is roughly the time expected to startup a Python
process. But it precludes writing code that, for example, looping over the billion
reads in an RNA-seq dataset.

The data transfer time, $I$, is the time it takes to transfer a Mb of data
between nodes. This time includes the costs of transforming data from storage
formats to native data structures. $I$ can be a bottleneck with large datasets
need to be passed between many nodes that do relatively small operations on
them. Passing a dataset from one function to another function by reference is a
nearly zero cost operation. This is supported for `morloc` within-language
calls. For foreign calls, `morloc`, reformats that data, copies it into shared
memory, and sends the relative pointer to the foreign language server. In
Nextflow, data is streamed from one process to the next. Streaming is fairly
efficient, and Nextflow is interop in this benchmark (where the data is just a
long string that requires to validation or formatting) is nearly as fast as
`morloc` data transfer.


The parameters of this model can be calculated by measuring the runtime of a
simple linear pipeline. I am using the python program `hyperfine` for
benchmarking the runtime of the CLI programs. This can be pretty noisy,
especially for Nextflow with is very long startup time. The benchmarking
pipeline starts with a text file of varying size (from 0Mb to 160Mb). The file
is loaded and then passed through a chain of functions that each performa a
small operation on it. This operation requires copying the entire string and
modifying it. I can estimate all the parameters in the runtime model with four
benchmarks:

 1. Estimate $L$ by running single-language benchmarks (no manager) that loads
    datasets of different sizes but sets the number of iterations (nodes in the
    loop) to 0. This means data is loaded and then nothing is done with
    it. Using linear regression, I calculate the slope of time versus
    filesize. The slope equals the $L$ parameter for the given language (e.g.,
    R, Python, C++).

 2. Estimate $R$ by running a series of single-language benchmarks with constant
    filesize but varying number of iterations. Now the slope will be equal to
    the runtime cost per Mb of input.

 3. To estimate $Q$ and $S$, I set the data size, $k$, to 0 and benchmark
    runtimes for number of iteration, $n$, from 0 to 16. The intercept is $S$,
    the pipeline startup cost, and the slope is $Q$, the node startup cost.  

$$
t(n,k=0) = S + Lk + n(Q + Rk + Ik) = S + nQ
$$

 4. To estimate $I$, we set the number of iterations to $n=4$ and let
    the data size, $k$, vary. We can then use the previously estimated values
    for $S$, $Q$, $L$, and $R$, along with the estimated intercept and slope from the
    benchmarks, to solve for $I$, as below:

$$
t(n=20,k) = S + Lk + n(Q + Rk + Ik)
          = S + Lk + 4Q + 4Rk + 4Ik
          = (S + 4Q) + (L + 4R + 4I)k
$$

$$
I = (slope - L - 4R) / 4
$$

## Estimate $L$ - data loading cost per Mb

Set $n=0$ for single-language implementations. Each of these runs loads the
input file and then quits. The `morloc`, `snakemake`, and `nextflow` programs
use the same code for reading input, so the obtained coefficients should be
transferable. The runtime is:

$$
t_{n=0} = S_{lang} + L k
$$

Now $L$ can be calculated as the slope for set of benchmarks with constant $n$
and varying $k$ (input data size, ranging from 10Mb to 160MB). The intercept,
$S_{lang}$, is the startup time for the single-language program (not
particularly interesting to us).

```{r, echo=FALSE}
Ldf <- df %>%
    filter(mode == "loading") %>%
    filter(load_lang %in% c("c", "r", "p")) %>%
    arrange(k)

Ltab <- Ldf %>%
  group_by(load_lang) %>%
  do(model = lm(t ~ k, data = .)) %>%
  summarise(
    category = load_lang,
    intercept = coef(model)[1],
    coefficient = coef(model)[2],
    r_squared = summary(model)$r.squared
  )

L <- Ltab$coefficient
names(L) <- Ltab$category

Lplot <- ggplot(Ldf, aes(x = k, y = t, color = lang)) +
    geom_smooth(method = "lm", se = FALSE, alpha=0.1) +
    ggpmisc::stat_poly_eq(
        aes(label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    geom_point(size = 4) +
    scale_color_manual(values = c(
        "C" = "#555555",
        "R" = "#198CE7",
        "python" = "#FFA500"
    )) +
    labs(
        x = "k [Mb]",
        y = "t(k=x, n=0) [s]",
        title = "Estimating S (k=x, n=0)"
    ) +
    theme_minimal() +
    theme(legend.position = "none")

```



## Estimate $R$ - functional runtime cost per Mb

In the single-language implementations, set the number of loops to $n=20$ and
run with variable size data. This allows the estimation of $R$, the runtime. All
workflow languages use the same functions for in each loop, so $R$ should be
transferable.

$$
t_{n=4} = S_{lang} + (L + Rn) k
$$

The slope is file loading, $L$, time plus $n$ times the node run time, $R$. So
$R$ can be calculated from the slope as follows:

$$
R = (\beta_{1} - L) / n
$$

```{r, echo=FALSE}
Rdf <- df %>%
    filter(n == 20) %>%
    filter(run_lang %in% c("c", "r", "p")) %>%
    arrange(k)

Rtab <- Rdf %>%
  group_by(run_lang) %>%
  do(model = lm(t ~ k, data = .)) %>%
  summarise(
    category = run_lang,
    intercept = coef(model)[1],
    coefficient = coef(model)[2],
    r_squared = summary(model)$r.squared
  )

Rtab$L <- sapply(Rtab$category, function(lang) L[lang])
Rtab$R <- (Rtab$coefficient - L) / 20

R <- Rtab$R
names(R) <- Rtab$category

Rplot <- ggplot(Rdf, aes(x = k, y = t, color = lang, group = lang)) +
    geom_smooth(method = "lm", se = FALSE, alpha=0.1) +
    ggpmisc::stat_poly_eq(
        aes(label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    geom_point(size = 4) +
    geom_point(size=4) +
    scale_color_manual(values = c(
        "C" = "#555555",
        "R" = "#198CE7",
        "python" = "#FFA500"
    )) +
    labs(
        x = "k [Mb]",
        y = "t(k=x, n=20) [s]",
        title = "Estimating R (k=x, n=20)"
    ) +
    theme_minimal() +
    theme(legend.position = "bottom")

```

This is a rather round-about way of calculating $R$. A more direct solution
would be to just measure $R$ directly by timing the loop in each language
implementation. XXX - DO THIS AND COMPARE THE RESULTS TO THE REGRESSION RESULT.


```{r, out.width="100%", echo=FALSE, fig.cap="Fig 1. Benchmarks used to estimate L (load time) and R (function runtime). L corresponds directly to the slope of plot A. R is equal to the slope from plot B minus L and divided by n=20. R is the cost of running each function per Mb of input."}
(Lplot + Rplot) +
  patchwork::plot_annotation(
    tag_levels = "A"
  ) &
  theme(
    plot.tag = element_text(size = 12, face = "bold")
  )
```

```{r, echo=FALSE}
Rtab[, c("L", "R")] %>%
  kable(caption = "Table 1. L and R estimates",
        format = "html",
        position = "bottom",
        digits = 4,
        col.names = c("Load Time (L) [s/Mb]", "Runtime (R) [s/Mb]")) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                full_width = FALSE)

```



## Estimate $Q$ and $S$ - cost of starting a pipeline and a single node

Set $k=0$ and estimate the constant cost of making an interop call. Here we
compare all of our workflow languages when data of 0 length is passed
between them. This measures the overhead of a call between nodes.

$$
t(n,k=0) = S + Lk + n(Q + Rk + Ik) = S + nQ
$$

The runtime is equal to pipeline startup time, $S$, plus the number of loops,
$n$, times the node start cost, $Q$. So benchmarking at many different values
for $n$, we can estimate $S$ as the intercept and $Q$ as the slope coefficient.


```{r, out.width="100%", fig.asp=1.5, echo=FALSE, fig.cap="Fig 2", warning=FALSE}
QSdf <- df %>%
    filter(lang %in% c("morloc", "snakemake", "nextflow")) %>%
    mutate(group = paste0(lang, "_", mode)) %>%
    filter(! group == "snakemake_testlt") %>%
    filter(k == 0) %>%
    # Remove the ultra-long running pc and cp comparisons
    filter(n <= 16) %>%
    group_by(group) %>%
    arrange(n) %>%
    # stupid hack to avoid the overlap between two benchmark sets
    mutate(
        prev_t = lag(t),
        next_t = lead(t),
        diff = abs(t - (prev_t + next_t) / 2)
    ) %>%
    group_by(group, n, k) %>%
    slice(which.min(diff)) %>%
    select(-prev_t, -next_t, -diff) %>%
    ungroup() %>%
    arrange(n)

QSdf_nonmorloc <- QSdf %>%
    filter(lang != "morloc")

QSdf_morloc <- QSdf %>%
    filter(lang == "morloc")

Qtab <- QSdf %>%
  group_by(group) %>%
  do(model = lm(t ~ n, data = .)) %>%
  summarise(
    category = group,
    intercept = coef(model)[1],
    coefficient = coef(model)[2],
    r_squared = summary(model)$r.squared
  )

Q <- Qtab$coefficient
names(Q) <- Qtab$category

S <- Qtab$intercept
names(S) <- Qtab$category

QSPlot <- ggplot() +
    geom_smooth(
        data = QSdf_nonmorloc,
        aes(x = n, y = t, color = lang, group = group),
        method = "lm",
        se = FALSE,
        alpha=0.1,
    ) +
    ggpmisc::stat_poly_eq(
        data = QSdf_nonmorloc,
        aes(x = n, y = t, group = group, color = lang, label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    geom_point(data = QSdf_nonmorloc, aes(x = n, y = t, color = lang, group = group), size=2) +
    geom_path(data = QSdf_morloc, aes(x = n, y = t, color = lang, group = group)) +
    scale_color_manual(values = c(
        "nextflow" = "blue",
        "snakemake" = "dark green",
        "morloc" = "orange"
    )) +
    labs(x = "Number of calls", y = "Time (s)", title = "size=0, ncalls=n") +
    theme_minimal() +
    theme(legend.position = "bottom")


 # The extremely high startup costs of Nextflow and Snakemake render the graph hard
 # to visualize. We can subtract the intercept from each line to offset this:
QSdf_nonmorloc_qtab <- merge(QSdf_nonmorloc, Qtab, by.x = "group", by.y = "category") %>%
    dplyr::mutate(t_adj = t - intercept) %>%
    arrange(n)

QSdf_morloc_qtab <- merge(QSdf_morloc, Qtab, by.x = "group", by.y = "category") %>%
    dplyr::mutate(t_adj = t - intercept) %>%
    arrange(n)

QSPlot_norm <- ggplot() +
    geom_smooth(
        data = QSdf_nonmorloc_qtab,
        aes(x = n, y = t_adj, color = lang, group = group),
        method = "lm",
        se = FALSE,
        alpha=0.1
    ) +
    ggpmisc::stat_poly_eq(
        data = QSdf_nonmorloc_qtab,
        aes(x = n, y = t_adj, group = group, color = lang, label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    geom_point(data = QSdf_nonmorloc_qtab, aes(x = n, y = t_adj, color = lang, group = group), size=2) +
    geom_path(data = QSdf_morloc_qtab, aes(x = n, y = t_adj, color = lang, group = group)) +
    scale_color_manual(values = c(
        "nextflow" = "blue",
        "snakemake" = "dark green",
        "morloc" = "orange"
    )) +
    labs(x = "Number of calls", y = "Time (s)", title = "size=0, ncalls=n") +
    theme_minimal() +
    theme(legend.position = "bottom")

 # The morloc runs are so fast relative to Nextflow and Snakemake, that we still
 # need to zoom in to distinguish them.
QSPlot_norm_morloc <-
    ggplot(QSdf_morloc_qtab) +
    geom_smooth(
        aes(x = n, y = t_adj, color = group, group = group),
        method = "lm",
        se = FALSE,
        alpha=0.1
    ) +
    ggpmisc::stat_poly_eq(
        aes(x = n, y = t, group = group, color = group, label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    scale_color_manual(values = c(
        "morloc_cr" = "#AA2222",
        "morloc_pr" = "#880000",
        "morloc_rp" = "#FF0000",
        "morloc_rc" = "#00FFFF",
        "morloc_pc" = "#00FF00",
        "morloc_cp" = "#0000FF",
        "morloc_cc" = "#000000",
        "morloc_pp" = "#444444",
        "morloc_rr" = "#888888"
    )) +
    geom_point(aes(x = n, y = t_adj, color = group, shape = run_lang), size=2) +
    labs(x = "Number of calls", y = "Time (s)", title = "Just morloc; size=0, ncalls=n") +
    theme_minimal() +
    theme(legend.position = "bottom")


QSPlot_norm_morloc_pc <- df %>%
  filter(n > 16) %>%
  filter(mode %in% c("pc", "cp")) %>%
  arrange(n) %>%
  ggplot(aes(x = n, y = t, color = mode)) +
    geom_path(aes(group = mode)) +
    geom_point(size = 4) +
    geom_smooth(method = "lm", se = FALSE) +
    scale_color_manual(values = c(
        "pc" = "blue",
        "cp" = "orange"
    )) +
    ggpmisc::stat_poly_eq(
        aes(label = ..eq.label..),
        formula = y ~ x,
        parse = TRUE,
        coef.digits = 3
    ) +
    labs(x = "Number of calls", y = "Time (s)", title = "Python-C interop cost; size=0, ncalls=n") +
    theme_minimal() +
    theme(legend.position = "bottom")


QSCombined <- ((QSPlot + QSPlot_norm) / (QSPlot_norm_morloc + QSPlot_norm_morloc_pc)) +
  patchwork::plot_annotation(
    tag_levels = "A"
  ) &
  theme(
    plot.tag = element_text(size = 12, face = "bold")
  )

QSCombined
```

We see R interop is quite slow, relative to Python. This is may be a
surmountable implementation issue. Python and C interop is limited by the speed
messages can be passed over UNIX domain sockets.

Running benchmarks for larger numbers of iterations with just Python and C
interop let's us make a better estimate of their interop:

The overhead of a single function call in a one-language morloc program is
beyond the resolution limits of our benchmark. There is some overhead, since
morloc, as currently implemented, creates wrappers around the functions.




## Estimate $R$ and $I$ by setting n=20, k=x

Repeating the main runtime equation and definitions:

$$
t(n,k) = S + Lk + n(Q + Rk + Ik)
$$

where
 * $t$ - (s) pipeline runtime
 * $n$ - (unit) number of nodes in the pipeline
 * $k$ - (Mb) size of the data that is passed between each node (assuming data size is the same between all nodes)
 * $S$ - (s) the one-time cost for initializing the pipeline
 * $L$ - (s/Mb) the one-time cost for loading an initial data file of size `k`
 * $I$ - (s/Mb) the cost for moving data to/from a node per Mb
 * $Q$ - (s) the constant cost of starting a node
 * $R$ - (s/Mb) the runtime of running node function per Mb of input

Now we set $n=4$

$$
t(n=4,k) = S + Lk + 4(Q + Rk + Ik)
$$

And subsitute in the intercept and slope for our benchmarks:

$$
t(k | n=4) = b0 + b1 k
$$

where the intercept is ($b0 = S + 4Q$) and the slope is ($b1 = L + 4R + 4I$)

We have already estimated $L$ and $R$, so now we can estimate $I$ (the cost of
moving data to/from a node per Mb).

$$
I = (b1 - L - 4R) / 4
$$


```{r, echo=FALSE}
ipred_df <- df %>%
    dplyr::filter(k != 0) %>%
    dplyr::filter(n == 4) %>%
    dplyr::arrange(k) %>%
    mutate(group = paste0(lang, "_", mode))

ipred_stat <- ipred_df %>%
    group_by(group) %>%
    do(model = lm(t ~ k, data = .)) %>%
    summarise(
      category = group,
      intercept = coef(model)[1],
      coefficient = coef(model)[2],
      r_squared = summary(model)$r.squared
    )

ipred_stat <- ipred_df %>%
    select(group, load_lang, run_lang) %>%
    dplyr::distinct() %>%
    merge(ipred_stat, by.x = "group", by.y = "category") %>%
    mutate(
        L = sapply(load_lang, function(l) L[l]),
        R = sapply(run_lang, function(l) R[l])
    ) %>%
    mutate(I = (coefficient - L - 4 * R) / 4) %>%
    arrange(I)


ipred_stat[, c("group", "I")]

all_size_df <- df %>%
    filter(!grepl("data-00MB", command)) %>%
    filter(mode != "loading") %>%
    filter( ((lang == "snakemake") & (mode == "testlc")) |
            ((lang == "morloc") & (mode %in% c("pc", "cc", "cp"))) |
            (lang %in% c("nextflow", "C", "R", "python"))
          ) %>%
    mutate(group = ifelse(lang == "morloc", paste0("morloc_", mode), lang)) %>%
    mutate(size = k * 1e7) %>%
    arrange(size)

ggplot(all_size_df) +
    geom_path(aes(x = k, y = t, color = group, group = group)) +
    geom_point(aes(x = k, y = t, color = group), size=4) +
    labs(x = "Size (Mb)", y = "Time (s)", title = "size=n, ncalls=4") +
    theme_minimal()
```

```{r, echo=FALSE}
Qtab %>%
    mutate(S = intercept, Q = coefficient, group = category) %>%
    merge(y = ipred_stat, by = "group") %>%
    mutate(
        S = S*1000,
        Q = Q*1000,
        I = I*1000
    ) %>%
    select(group, S, Q, I, R, L) %>%
    arrange(group) %>%
    kable(caption = "Table 2. S and Q estimates",
          format = "html",
          position = "bottom",
          col.names = c("group", "S: program start [ms]", "Q: node start [ms]", "I: transfer cost [ms/Mb]", "R: node runtime [ms]", "L: load time [ms/Mb]")
    ) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  full_width = FALSE)
```



```{r, echo=FALSE}
Qtab %>%
    mutate(S = intercept, Q = coefficient, group = category) %>%
    merge(y = ipred_stat, by = "group") %>%
    mutate(
        S_rel = max(S) / S,
        Q_rel = max(Q) / Q,
        I_rel = ifelse(I > 0, max(I) / I, NA),
        R_rel = max(R) / R,
        L_rel = max(L) / R
    ) %>%
    select(group, S_rel, Q_rel, I_rel, R_rel, L_rel) %>%
    arrange(group) %>%
    kable(caption = "Table 3. S and Q estimates as speed multiples of the worst",
          format = "html",
          position = "bottom",
          col.names = c("group", "S: program start [s]", "Q: node start [s]", "I: transfer cost [s/Mb]", "R: node runtime [s]", "L: load time [s]")
    ) %>%
    kable_styling(bootstrap_options = c("striped", "hover", "condensed"),
                  full_width = FALSE)
```
